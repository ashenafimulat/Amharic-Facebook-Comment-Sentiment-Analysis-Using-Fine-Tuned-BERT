Amharic Facebook Comment Sentiment Analysis Using Fine-Tuned BERT
This repository contains the code and resources for a sentiment analysis project focused on Amharic Facebook comments. The project leverages a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model to classify the sentiment of comments as strongly positive, positive, strongly negative, negative, or neutral. Amharic, being a low-resource language, presents unique challenges in natural language processing (NLP), and this project aims to address those challenges by adapting state-of-the-art transformer models.

Key Features
Fine-Tuned BERT Model: A pre-trained BERT model is fine-tuned on a custom dataset of Amharic Facebook comments to perform sentiment analysis.

Amharic Language Support: The project focuses on sentiment analysis for Amharic text, addressing the scarcity of NLP resources for this language.

Dataset: Includes a curated dataset of Amharic Facebook comments labeled with sentiment (strongly positive, positive, strongly negative, negative, or neutral).

Preprocessing: Tools for cleaning and preprocessing Amharic text to improve model performance.

Evaluation: Comprehensive evaluation metrics (accuracy, precision, recall, F1-score) to assess model performance.
